# MCAI Configuration

[models]
# Directory containing GGUF models
directory = "./models"

[inference]
# Default temperature for text generation (0.0-1.0)
temperature = 0.7
# Maximum number of tokens to generate
max_tokens = 512
# Context window size
context_size = 2048
# Number of layers to offload to GPU (0 = CPU only). Automatically uses Metal on macOS if > 0.
# Setting to a high number like 99 tells llama.cpp to offload as many as possible.
n_gpu_layers = 99
# Use memory mapping (mmap) if possible (reduces initial RAM usage).
use_mmap = true
# Force the system to keep the model in RAM (mlock). Requires sufficient RAM.
use_mlock = false

[server]
# Server host address
host = "127.0.0.1"
# Server port
port = 8080
# Enable rate limiting
rate_limit = false
# Maximum requests per minute
rate_limit_rpm = 60

[logging]
# Logging level (error, warn, info, debug, trace)
level = "info"
# Log file path (optional)
file = "mcai.log"