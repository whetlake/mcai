# MCAI Configuration

[models]
# Directory containing GGUF models
directory = "./models"

[inference]
# Default temperature for text generation (0.0-1.0)
temperature = 0.7
# Maximum number of tokens to generate
max_tokens = 2048
# Context window size
context_size = 4096

[server]
# Server host address
host = "127.0.0.1"
# Server port
port = 3000
# Enable rate limiting
rate_limit = true
# Maximum requests per minute
rate_limit_rpm = 60

[logging]
# Logging level (error, warn, info, debug, trace)
level = "info"
# Log file path (optional)
file = "mcai.log"